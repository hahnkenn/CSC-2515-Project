{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_code.ipynb","provenance":[],"collapsed_sections":["PaiEZzpxNW4w"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkP6csgdtZuM","executionInfo":{"status":"ok","timestamp":1608081327348,"user_tz":300,"elapsed":340,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"19e594c2-f8f3-4e9d-a0cd-69d24d5a3696"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PDfkoymdCFUo","executionInfo":{"status":"ok","timestamp":1608081327844,"user_tz":300,"elapsed":241,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["import os\n","os.chdir('/content/drive/MyDrive/CSC2515 Project Part 2') # change directory to project folder "],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9zo0cZoLGg-2","executionInfo":{"status":"ok","timestamp":1608081328700,"user_tz":300,"elapsed":335,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"764e36ed-1ee3-4c4e-f1e5-a2439a6a1ace"},"source":["os.getcwd()"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/CSC2515 Project Part 2'"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"E4O7NTxUn8Z0"},"source":["## import libraries"]},{"cell_type":"code","metadata":{"id":"LmcrM-ujiSGx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608081333461,"user_tz":300,"elapsed":2916,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"012f4b8f-fff7-4fb8-ed7f-39114dd4f5a8"},"source":["#!pip uninstall bert-tensorflow\n","!pip install bert-for-tf2 # bert for tensorflow 2.0 (NOT 1.0 - that's the tf version that causes headaches for tokenizer.py in BERT)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.7)\n","Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n","Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2frGL6BCdwpA","executionInfo":{"status":"ok","timestamp":1608081333462,"user_tz":300,"elapsed":1768,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# mount drive , then do:\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf # version 2.3.0 required\n","import matplotlib.pyplot as plt\n","\n","import tensorflow_hub as hub\n","from datetime import datetime\n","from tensorflow.keras.models import Model\n","from sklearn.model_selection import train_test_split\n","import bert\n","from bert.tokenization import bert_tokenization\n","%matplotlib inline\n","\n","from pathlib import Path # path for .TFRecord files\n","import collections # for 2 bert preprocessing functions\n","\n","os.chdir('/content/drive/MyDrive/CSC2515 Project Part 2') # change directory to project folder"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Nn7TCgS5hkK","executionInfo":{"status":"ok","timestamp":1608081333463,"user_tz":300,"elapsed":322,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["os.chdir('bert')\n","import modeling # manually import modeling.py script on drive - reference: https://github.com/google-research/bert/blob/master/modeling.py"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A8lNMudnoE4V"},"source":["## Define Bert files & import train data"]},{"cell_type":"markdown","metadata":{"id":"5758OTxZwmLa"},"source":["note: we will be using the pre-trained 'BERT-base, uncased' model i.e. `uncased_L-12_H-768_A-12` downloaded at the source code repo README link [here](https://github.com/google-research/bert), with documentation. \n","\n","About `uncased_L-12_H-768_A-12` model: \n","12-layer, 768-hidden, 12-heads, 110M parameters"]},{"cell_type":"markdown","metadata":{"id":"gZI4zTt67O1h"},"source":["In the 'bert' subfolder: \n","- A TensorFlow checkpoint (bert_model.ckpt) containing the pre-trained weights (which is actually 3 files).\n","- A vocab file (vocab.txt) to map subwords in BERT vocab to word indices in comment text.\n","- A config file (bert_config.json) which specifies the hyperparameters of the model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"qZiKi4nq7FgP","executionInfo":{"status":"ok","timestamp":1608081334933,"user_tz":300,"elapsed":332,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"b398cd43-8785-4865-847b-88e346fa44c5"},"source":["os.chdir('..')\n","os.getcwd()"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/CSC2515 Project Part 2'"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"IZt2O8pReY57","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1608081336600,"user_tz":300,"elapsed":1384,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"74142b15-51dc-4311-8f59-4947673552ee"},"source":["# import train data (note: won't use test.csv since has no true labels)\n","trainSet = pd.read_csv('train.csv')\n","trainSet.head(3)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","\n","[3 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"Hq1-7qEPqDKY","executionInfo":{"status":"ok","timestamp":1608081337264,"user_tz":300,"elapsed":639,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["vocab= 'bert/vocab.txt'\n","checkpoint = 'bert/bert_model.ckpt'\n","config_file = 'bert/bert_config.json'"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPOVKHr0xHER","executionInfo":{"status":"ok","timestamp":1608081337741,"user_tz":300,"elapsed":530,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["bert_tokenization.validate_case_matches_checkpoint(True,checkpoint) # check\n","\n","# initialize BERT tokenizer (which expects lower case words only)\n","tokenizer = bert_tokenization.FullTokenizer(vocab_file=vocab, do_lower_case=True)"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S-_qSJ8ADOBF"},"source":["## Data preprocessing \n"]},{"cell_type":"markdown","metadata":{"id":"bVJTBpPZGNNO"},"source":["### Split train/test data"]},{"cell_type":"code","metadata":{"id":"Y4dYr7AB8-w3","executionInfo":{"status":"ok","timestamp":1608081339419,"user_tz":300,"elapsed":499,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["id = 'id' # user ID\n","comment = 'comment_text' # corpus of comments\n","classes = ['toxic','severe_toxic','obscene','threat','insult','identity_hate'] # labels"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtPP8NXgTX6Z","executionInfo":{"status":"ok","timestamp":1608081340120,"user_tz":300,"elapsed":503,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# define train/test ratio\n","split = 0.9"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"-r-LppJl_9Wy","executionInfo":{"status":"ok","timestamp":1608081341782,"user_tz":300,"elapsed":287,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# perform 90/10 train/test split\n","length = trainSet.shape[0]\n","splitIndex = int(split*length)\n","train = trainSet[:splitIndex] # train set\n","test = trainSet[splitIndex:] # test set"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"AJzFH8Y-G8T2","executionInfo":{"status":"ok","timestamp":1608081342479,"user_tz":300,"elapsed":291,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"2d2b0f27-5976-45df-c958-585b45dc00c6"},"source":["# check \n","train.head()\n","# train.shape"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id  ... identity_hate\n","0  0000997932d777bf  ...             0\n","1  000103f0d9cfb60f  ...             0\n","2  000113f07ec002fd  ...             0\n","3  0001b41b1c6bb37e  ...             0\n","4  0001d958c54c6e35  ...             0\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"tHgIrTQ1wc7-","executionInfo":{"status":"ok","timestamp":1608081343807,"user_tz":300,"elapsed":269,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# the below 2 functions (docstrings) are from BERT's source code 'run_classifier.py' here (https://github.com/google-research/bert/blob/master/run_classifier.py)\n","# convert each row in df from `InputExample`instance -> `InputFeatures` instance to feed to BERT\n","class InputExample(object):\n","    \"\"\"A single training/test example (e.g. single row of df as separate elements like `guid`, `text_a` etc.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, labels=None):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example (= None in our case)\n","            text_a: string. The untokenized text of the first sequence. \n","            text_b: don't need - only used for sequence *pair* (i.e. compare 2 sentences rather than only use 1 for classification) tasks.\n","            labels: (Optional) [string]. The label of the example. \n","        \"\"\"\n","        self.guid = guid # ID\n","        self.text_a = text_a # comment\n","        self.text_b = text_b # None\n","        self.labels = labels # list of toxicity labels\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids,\n","        self.is_real_example=is_real_example # True, flags example as valid row in df (for BERT real token masking later)"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJ3fToMCA-5o","executionInfo":{"status":"ok","timestamp":1608081346536,"user_tz":300,"elapsed":297,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# The following function is adapted from the 'create_examples()' function from the 'run_classifier.py' source code https://github.com/google-research/bert/blob/master/run_classifier.py\n","#  Because need to accommodate for the multilabel (6 toxicity classes), and 1-hot encoded columns.\n","def createBertInput(data, labelsExist=True):\n","    \"\"\"Creates examples for train set with labels (examples = a format understandable by BERT )\n","    Return: a list of class InputExample instances, each corresponding to a row in the `df` (as argument)\n","    \"\"\"\n","    input = [] # init. list\n","    # for each row index (i), and each row (line) of df as a string-delimited list\n","    for (i, line) in enumerate(data.values):\n","        # assuming labels available (for train data)\n","        if labelsExist:\n","            labels = line[2:] # get toxicity labels for that row \n","        else:\n","            labels = [0,0,0,0,0,0] # if no labels then assume clean comment\n","        guid = line[0] # extract'id' for that row\n","        text_a = line[1] # extract 'comment' for that row\n","\n","        input.append(InputExample(guid=guid, text_a=text_a, labels=labels)) # append as InputExample to list\n","    return input"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"orghuP74ItCR","executionInfo":{"status":"ok","timestamp":1608081348913,"user_tz":300,"elapsed":802,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# create InputExamples()\n","trainInput = createBertInput(train) # a list of 'InputExample` class instances, len = train"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sqCwrZTbkb3c"},"source":["### Below 2 functions take input embeddings (i.e. examples) and converts them to features usable by BERT. \n","**Features = a combination of :** \n","\n","a) token embeddings  (tokenized words, with [CLS] and [SEP] tokens added to signal beginning & ending of a phrase)\n","\n","b) segmentation embeddings (akak *type or segment IDs*)  \n","\n","c) position embeddings (e.g. where word is in a sentence) "]},{"cell_type":"code","metadata":{"id":"pw_dzwWsT39W","executionInfo":{"status":"ok","timestamp":1608081350637,"user_tz":300,"elapsed":259,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# Preprocessing params\n","maxSeqLength = 128 # max number of tokens in a tokenized comment"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRQhfrNzEUxG","executionInfo":{"status":"ok","timestamp":1608081351202,"user_tz":300,"elapsed":248,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# taken from source code https://github.com/google-research/bert/blob/master/run_classifier.py\n","# if don't include might throw errors when padding tokens\n","class PaddingInputExample(object): \n","    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","    When running eval/predict on the GPU, we need to pad the number of examples\n","    to be a multiple of the batch size, because the GPU requires a fixed batch\n","    size. The alternative is to drop the last batch, which is bad because it means\n","    the entire output data won't be generated.\n","    We use this class instead of `None` because treating `None` as padding\n","    batches could cause silent errors.\n","    \"\"\""],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_8bnV_tgLxF"},"source":["Below 2 functions adapted from source code https://github.com/google-research/bert/blob/master/run_classifier.py \n","\n","Changes made: 1) omitted `label_list` argument since already in `example` argument 2) omitted `tokens_b` cases\n","3) added label_list to append to `inputFeature`"]},{"cell_type":"code","metadata":{"id":"RSRIAwtDEauZ","executionInfo":{"status":"ok","timestamp":1608081354391,"user_tz":300,"elapsed":274,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["def convert_single_example(ex_index, example, max_seq_length, tokenizer):\n","    \"\"\"Convert single `InputExample()` -> single `InputFeature()`.\"\"\"\n","    # for non-InputExamples\n","    if isinstance(example, PaddingInputExample):\n","        return InputFeatures(\n","            input_ids=[0] * max_seq_length,\n","            input_mask=[0] * max_seq_length,\n","            segment_ids=[0] * max_seq_length,\n","            label_ids=0,\n","            is_real_example=False)\n","    # tokenizes single comment\n","    tokens_a = tokenizer.tokenize(example.text_a)\n","    tokens_b = None # remove tokens_b cases since sequence pair classification not apply\n"," \n","    # '-2' because [CLS] and [SEP] tokens added to each example comment\n","    if len(tokens_a) > max_seq_length - 2:\n","        tokens_a = tokens_a[0:(max_seq_length - 2)] # cut token short to be same length \n","\n","    tokens = []\n","    segment_ids = []\n","    tokens.append(\"[CLS]\") # marks beginning of token\n","    segment_ids.append(0) # add '0' to segment_id for [CLS] token\n","    for token in tokens_a:\n","        tokens.append(token)\n","        segment_ids.append(0) # for each token in tokens_a, add '0' to segment_ids (list), so len(segment_ids) == len(tokens)\n","    tokens.append(\"[SEP]\") # marks end of token\n","    segment_ids.append(0) # add '0' to segment_id for [SEP] token\n","    \n","    # convert from token (list of strings) -> a list of numbers\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    # input mask: 1 = REAL tokens, 0 = padding tokens\n","    input_mask = [1] * len(input_ids) \n","\n","    # add zero padding until reach max_seq_length.\n","    while len(input_ids) < max_seq_length:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        segment_ids.append(0)\n","    # check\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","    # append labels for that example\n","    labels = []\n","    for tox_label in example.labels:\n","        labels.append(int(tox_label))\n","\n","    # create 'InputFeatures()' instance for the single example\n","    feature = InputFeatures(\n","        input_ids=input_ids, # a list of numbers , each number represents a word in the tokenized comment\n","        input_mask=input_mask, # a vector of 1's\n","        segment_ids=segment_ids, # a vector of 0's\n","        label_ids=labels, # a vector of labels (length 6, since 6 toxicity classes) for that example e.g. [1, 0, 1, 0, 0, 0]\n","        is_real_example=True) # flag\n","    return feature"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"epEmjXDiG6Z1","executionInfo":{"status":"ok","timestamp":1608081357797,"user_tz":300,"elapsed":260,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# calls above function to create single `InputFeature` instance\n","def file_based_convert_examples_to_features(\n","        examples, max_seq_length, tokenizer, output_file):\n","    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n","\n","    writer = tf.io.TFRecordWriter(output_file) # init. writer to write to tf.record file\n","\n","    # for each index, example: \n","    for (ex_index, example) in enumerate(examples):\n","        # convert single example -> feature (`InputFeature()` instance )\n","        feature = convert_single_example(ex_index, example,\n","                                         max_seq_length, tokenizer)\n","\n","        def create_int_feature(values):\n","          ''' Convert features to TFRecord format\n","          '''\n","          f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n","          return f\n","\n","        # initialize ordered dictionary e.g. {'input_ids': vector of input_ids, 'input_mask', vector of input_mask, ...}\n","        features = collections.OrderedDict()\n","        # append embeddings and labels to dict\n","        if isinstance(feature.label_ids, list): # check if list\n","            labels_feat = feature.label_ids\n","        else:\n","            labels_feat = feature.label_ids[0]\n","        features[\"label_ids\"] = create_int_feature(labels_feat)\n","        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n","        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n","        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n","        features[\"is_real_example\"] = create_int_feature(\n","            [int(feature.is_real_example)])\n","\n","        # convert `features` to type `tf.train.Example` (format suitable for tf.record file)\n","        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n","        # write to tf.record file\n","        writer.write(tf_example.SerializeToString())  \n","    writer.close()"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uKhU_Vj2GWLb"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"8y4fgJud5--i"},"source":["### Configure training files (.TFRecord)"]},{"cell_type":"code","metadata":{"id":"c9q6uFX-EgG9","executionInfo":{"status":"ok","timestamp":1608081556786,"user_tz":300,"elapsed":194927,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# .TFRecord file to store training output\n","trainFile = os.path.join('bert_output', \"train.tf_record\") # file path \n","\n","# create file if not exist in directory\n","if not os.path.exists(trainFile): \n","    open(trainFile, 'w').close()\n","\n","# write each `InputExample()` (from train df) as tf.Example into training.tf_record file\n","file_based_convert_examples_to_features(\n","            trainInput, maxSeqLength, tokenizer, trainFile) # file size: 'training.tf_record' should be ~81 MB"],"execution_count":57,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"netKyXlM62k6"},"source":["### Configure training function"]},{"cell_type":"code","metadata":{"id":"1eMM2ToEfFSU","executionInfo":{"status":"aborted","timestamp":1608068297633,"user_tz":300,"elapsed":737,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# below function taken from source code https://github.com/google-research/bert/blob/master/run_classifier.py \n","\n","def file_based_input_fn_builder(input_file, seq_length, is_training,\n","                                drop_remainder):\n","    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\n","    Returns: `input_fn` object\"\"\"\n","\n","    name_to_features = {\n","        \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n","        \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n","        \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n","        \"label_ids\": tf.io.FixedLenFeature([6], tf.int64), # specify 6 toxicity labels\n","        \"is_real_example\": tf.io.FixedLenFeature([], tf.int64),\n","    }\n","\n","    def _decode_record(record, name_to_features):\n","        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n","        example = tf.io.parse_single_example(record, name_to_features)\n","        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n","        # So cast all int64 to int32.\n","        for name in list(example.keys()):\n","            t = example[name]\n","            if t.dtype == tf.int64:\n","                t = tf.cast(t, tf.int32)\n","            example[name] = t\n","\n","        return example\n","\n","    def input_fn(params):\n","        \"\"\"The actual input function.\"\"\"\n","        batch_size = params[\"batch_size\"]\n","\n","        # For training, we want a lot of parallel reading and shuffling.\n","        # For eval, we want no shuffling and parallel reading doesn't matter.\n","        d = tf.data.TFRecordDataset(input_file)\n","        if is_training:\n","            d = d.repeat()\n","            d = d.shuffle(buffer_size=100)\n","\n","        d = d.apply(\n","              tf.data.experimental.map_and_batch(\n","                lambda record: _decode_record(record, name_to_features),\n","                batch_size=batch_size,\n","                drop_remainder=drop_remainder))\n","\n","        return d\n","\n","    return input_fn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkHc8Gn_EuO6","executionInfo":{"status":"aborted","timestamp":1608068297634,"user_tz":300,"elapsed":736,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["# define training function\n","trainFunction = file_based_input_fn_builder(\n","    input_file=trainFile,\n","    seq_length=maxSeqLength,\n","    is_training=True,\n","    drop_remainder=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AM7IWVDN8CEb"},"source":["### Functions to initialize BERT "]},{"cell_type":"markdown","metadata":{"id":"Vf3R7DZR8Iyq"},"source":["Below model functions **adapted** from *same* source code (https://github.com/google-research/bert/blob/master/run_classifier.py)\n","- Changes made to `create_model()` function : 1) labels casting to float32 since already one-hot encoded in train data 2) use sigmoid not softmax activation since labels NOT mutually exclusive\n","- Changes made to `model_fn_builder()` function: 1) omitted tpu argument & code (since gpu instance used) 2) changed logit and probability computations, added AUC scores for multilabel case"]},{"cell_type":"code","metadata":{"id":"EWt-_UqkTg6L"},"source":["os.chdir('bert')\n","import optimization # importing optimization.py from BERT source code\n","os.chdir('..')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7LqYDrTqNgm"},"source":["def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n","                 labels, num_labels, use_one_hot_embeddings):\n","    \"\"\"creates BERT model for text classification.\"\"\"\n","    model = modeling.BertModel(\n","        config=bert_config,\n","        is_training=is_training,\n","        input_ids=input_ids,\n","        input_mask=input_mask,\n","        token_type_ids=segment_ids,\n","        use_one_hot_embeddings=use_one_hot_embeddings) #creating the BERT model\n","\n","    output_layer = model.get_pooled_output() # \"pool\" model by taking hidden state corresponding\n","        # to the first token. Since model has been pre-trained.\n","\n","    #size of the last hidden layer\n","    hidden_size = output_layer.shape[-1]\n","    \n","    output_weights = tf.compat.v1.get_variable( #this function creates a new variable for tensorflow to work with\n","        \"output_weights\", [num_labels, hidden_size],\n","        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n","\n","    output_bias = tf.compat.v1.get_variable( # add bias term\n","        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","    with tf.compat.v1.variable_scope(\"loss\"): #apparently another way to create a variable\n","        if is_training:\n","            # 0.1 dropout regularization layer\n","            output_layer = tf.compat.v1.nn.dropout(output_layer, keep_prob=0.9)\n","\n","        logits = tf.matmul(output_layer, output_weights, transpose_b=True) #multiplying matrices output_layer and output_weights\n","        logits = tf.nn.bias_add(logits, output_bias) # adding bias to the logits matrix\n","        probabilities = tf.nn.sigmoid(logits) # use sigmoid not softmax since labels not mutually exclusive \n","        \n","        toxic_labels = tf.cast(labels, tf.float32) # labels already one-hot encoded # change to float32 compatible type\n","       \n","        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=toxic_labels, logits=logits) # for multilabel classification\n","        loss = tf.reduce_mean(per_example_loss) #computing the mean across the per_example_loss\n","\n","        return (loss, per_example_loss, logits, probabilities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWjshCK08aAc","executionInfo":{"status":"ok","timestamp":1608068280158,"user_tz":300,"elapsed":318,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}}},"source":["def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n","                     num_train_steps, num_warmup_steps, use_tpu, use_one_hot_embeddings):\n","    \"\"\"Returns `model_fn` closure for Estimator.\"\"\"\n","\n","    def model_fn(features, labels, mode, params):  \n","        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","        input_ids = features[\"input_ids\"]\n","        input_mask = features[\"input_mask\"]\n","        segment_ids = features[\"segment_ids\"]\n","        label_ids = features[\"label_ids\"]\n","        is_real_example = None\n","        if \"is_real_example\" in features:\n","             is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n","        else:\n","             is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n","\n","        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n","        \n","        # call above create_model() function\n","        (total_loss, per_example_loss, logits, probabilities) = create_model(\n","            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n","            num_labels, use_one_hot_embeddings)\n","        \n","        tvars = tf.compat.v1.trainable_variables() #tensorflow adds variables to its Graph\n","        initialized_variable_names = {}\n","        scaffold_fn = None\n","        if init_checkpoint: \n","            (assignment_map, initialized_variable_names\n","             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n","            if use_tpu: \n","                def tpu_scaffold():\n","                    tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map) \n","                    return tf.compat.v1.train.Scaffold()\n","                scaffold_fn = tpu_scaffold\n","            else: \n","                tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n","            \n","\n","        tf.compat.v1.logging.info(\"**** Trainable Variables ****\") # edit Dec 5\n","        for var in tvars:\n","            init_string = \"\"\n","            if var.name in initialized_variable_names:\n","                init_string = \", *INIT_FROM_CKPT*\"\n","\n","        output_spec = None\n","\n","        ## IF TRAIN\n","        if mode == tf.estimator.ModeKeys.TRAIN: \n","\n","            train_op = optimization.create_optimizer( \n","                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu) \n","\n","            output_spec = tf.estimator.EstimatorSpec( #defines the model to be run by Estimator\n","                mode=mode,\n","                loss=total_loss,\n","                train_op=train_op,\n","                scaffold=scaffold_fn)\n","        ## ELIF VALIDATION\n","        elif mode == tf.estimator.ModeKeys.EVAL: \n","            # modified `metric_fn()`\n","            def metric_fn(per_example_loss, label_ids, is_real_example, prob):\n","                sep_logits = tf.split(prob, num_labels, axis=-1) # split into subtensors\n","                sep_labels = tf.split(label_ids, num_labels, axis=-1) # ^\n","\n","                metrics_dict = {}\n","                # AUC per label class\n","                for ind, logit in enumerate(sep_logits):\n","                    label = tf.cast(sep_labels[ind], dtype=tf.int32)\n","                    auc1, auc2 = tf.compat.v1.metrics.auc(label, logit) # compute AUCs via riemann sum\n","                    metrics_dict[str(ind)] = (auc1, auc2)\n","                metrics_dict['test_loss'] = tf.compat.v1.metrics.mean(values=per_example_loss) # eval loss\n","                preds = tf.compat.v1.argmax(logit, axis=-1, output_type=tf.int32)\n","                preds = tf.reshape(preds, [tf.shape(probabilities)[0], -1])\n","                metrics_dict['accuracy'] = tf.compat.v1.metrics.accuracy(labels=label_ids, # eval accuracy\n","                                                                      predictions=preds, \n","                                                                      weights=is_real_example)\n","                return metrics_dict\n","\n","            eval_metrics = metric_fn(per_example_loss, label_ids,is_real_example,  probabilities)\n","            output_spec = tf.estimator.EstimatorSpec(\n","                mode=mode,\n","                loss=total_loss,\n","                eval_metric_ops=eval_metrics,\n","                scaffold=scaffold_fn)\n","        ## ELSE TEST\n","        else:\n","            output_spec = tf.estimator.EstimatorSpec(\n","                mode=mode,\n","                predictions={\"probabilities\": probabilities},\n","                scaffold=scaffold_fn)\n","        return output_spec\n","\n","    return model_fn"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"DYYwLc9muABw","executionInfo":{"status":"ok","timestamp":1607714712057,"user_tz":300,"elapsed":1555,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"ae2d56c0-a561-4729-a595-6343dd8ae383"},"source":["os.getcwd() "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/CSC2515 Project Part 2'"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"oZix5xDKFzPr"},"source":["### Define training hyperparameters, checkpoint parameters, and initialize model & tf.estimator\n"]},{"cell_type":"code","metadata":{"id":"qunfkm2r91Ya"},"source":["# CHECKPOINT PARAMS\n","checkpointEverySteps = 1000 # save checkpoint every 1000 steps\n","summaryEverySteps = 500\n","# created folder 'bert_output' in our shared drive folder to store model + checkpoints\n","output = \"bert_output\" # output directory created \n","# Specify output directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig( #this is set up for tf.estimator\n","    model_dir=output,\n","    save_summary_steps=summaryEverySteps,\n","    keep_checkpoint_max= 1 ,\n","    save_checkpoints_steps=checkpointEverySteps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbgTnSC3dZVm"},"source":["# HYPERPARAMETERS\n","epochs = 2.0\n","lr = 2e-5 # default \n","batchSize = 32 # suitable for gpu\n","warmup = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvOaQfQ3EqQb"},"source":["# compute no. of train steps & warmup steps \n","trainSteps = int(len(trainInput) / batchSize * epochs)\n","warmupSteps = int(trainSteps * warmup) # starts training w lower learning rate to help optimizer & attention mechanism "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68yPcWv8inqi","executionInfo":{"status":"ok","timestamp":1607707754018,"user_tz":300,"elapsed":7011,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"754fb430-deda-4c7d-d751-455f68accb55"},"source":["!pip install transformers\n","from transformers import BertConfig"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/db/98c3ea1a78190dac41c0127a063abf92bd01b4b0b6970a6db1c2f5b66fa0/transformers-4.0.1-py3-none-any.whl (1.4MB)\n","\r\u001b[K     |▎                               | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 25.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 24.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 18.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 17.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 13.9MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 15.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 15.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102kB 14.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112kB 14.2MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 14.2MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133kB 14.2MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143kB 14.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153kB 14.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163kB 14.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174kB 14.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 184kB 14.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 194kB 14.2MB/s eta 0:00:01\r\u001b[K     |████▉                           | 204kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 215kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 235kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 245kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████                          | 256kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 266kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 276kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 286kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 296kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 307kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 317kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 327kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████                        | 337kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 348kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 358kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 368kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 378kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 389kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 399kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 409kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████                      | 419kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 430kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 440kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 450kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████                     | 460kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 471kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 481kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 491kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 501kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 512kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 522kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 532kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 542kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 552kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 563kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 573kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 583kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 593kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 604kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 614kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 624kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 634kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 645kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 655kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 665kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 675kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 686kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 696kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 706kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 716kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 727kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 737kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 747kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 757kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 768kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 778kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 788kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 798kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 808kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 819kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 829kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 839kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 849kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 860kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 870kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 880kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 890kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 901kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 911kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 921kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 931kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 942kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 952kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 962kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 972kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 983kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 993kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.2MB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 14.2MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 46.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=268eebad1678e5ad0e271e1f19c64dcf1ebd7698a0d3e679c22c3cadf6500950\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_-qygLQ1EeN_"},"source":["# define model \n","bert_config = BertConfig.from_json_file(config_file) # get model params from downloaded BERT config file\n","\n","model_fn = model_fn_builder(\n","  bert_config=bert_config,\n","  num_labels= len(classes), # 6 toxicity classes\n","  init_checkpoint=checkpoint,\n","  learning_rate=lr,\n","  num_train_steps=trainSteps,\n","  num_warmup_steps=warmupSteps,\n","  use_tpu = False, \n","  use_one_hot_embeddings=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jqInSZPExW-","executionInfo":{"status":"ok","timestamp":1607707771336,"user_tz":300,"elapsed":1241,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"6bca8ee2-1c93-4934-ed68-bc0cb8541909"},"source":["# define estimator (for train and eval)\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": batchSize})"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'bert_output', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-TwgPa9TFZ1q"},"source":["### Begin training!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0YpRLISEvGG","executionInfo":{"status":"ok","timestamp":1607710385136,"user_tz":300,"elapsed":2610994,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"34a9a838-4f8e-42b9-8bd3-6ee80f8d9a7f"},"source":["print('Training Now...')\n","time_now = datetime.now()\n","estimator.train(input_fn=trainFunction, max_steps=trainSteps)\n","print(\"Training took \", datetime.now() - time_now)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","WARNING:tensorflow:From <ipython-input-35-b78e638bc912>:154: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From /content/drive/MyDrive/CSC2515 Project Part 2/bert/modeling.py:646: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.Dense instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","INFO:tensorflow:num_labels:6;logits:Tensor(\"loss/BiasAdd:0\", shape=(32, 6), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(32, 6), dtype=float32)\n","INFO:tensorflow:**** Trainable Variables ****\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n","INFO:tensorflow:Saving checkpoints for 0 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n","INFO:tensorflow:loss = 0.7033026, step = 0\n","INFO:tensorflow:global_step/sec: 2.96879\n","INFO:tensorflow:loss = 0.68629384, step = 100 (33.684 sec)\n","INFO:tensorflow:global_step/sec: 3.61747\n","INFO:tensorflow:loss = 0.64689976, step = 200 (27.644 sec)\n","INFO:tensorflow:global_step/sec: 3.61536\n","INFO:tensorflow:loss = 0.6169097, step = 300 (27.660 sec)\n","INFO:tensorflow:global_step/sec: 3.61526\n","INFO:tensorflow:loss = 0.6119835, step = 400 (27.660 sec)\n","INFO:tensorflow:global_step/sec: 3.26202\n","INFO:tensorflow:loss = 0.5778397, step = 500 (30.656 sec)\n","INFO:tensorflow:global_step/sec: 3.61331\n","INFO:tensorflow:loss = 0.47280702, step = 600 (27.676 sec)\n","INFO:tensorflow:global_step/sec: 3.61179\n","INFO:tensorflow:loss = 0.47981486, step = 700 (27.687 sec)\n","INFO:tensorflow:global_step/sec: 3.61384\n","INFO:tensorflow:loss = 0.36021635, step = 800 (27.672 sec)\n","INFO:tensorflow:global_step/sec: 3.61234\n","INFO:tensorflow:loss = 0.31370082, step = 900 (27.682 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 1000...\n","INFO:tensorflow:Saving checkpoints for 1000 into bert_output/model.ckpt.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 1000...\n","INFO:tensorflow:global_step/sec: 2.94104\n","INFO:tensorflow:loss = 0.26096758, step = 1000 (34.001 sec)\n","INFO:tensorflow:global_step/sec: 3.61349\n","INFO:tensorflow:loss = 0.20433824, step = 1100 (27.674 sec)\n","WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 1115 vs previous value: 1115. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n","INFO:tensorflow:global_step/sec: 3.61274\n","INFO:tensorflow:loss = 0.24511607, step = 1200 (27.680 sec)\n","INFO:tensorflow:global_step/sec: 3.61134\n","INFO:tensorflow:loss = 0.14706625, step = 1300 (27.691 sec)\n","INFO:tensorflow:global_step/sec: 3.61375\n","INFO:tensorflow:loss = 0.26268852, step = 1400 (27.672 sec)\n","INFO:tensorflow:global_step/sec: 3.61122\n","INFO:tensorflow:loss = 0.19105165, step = 1500 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.61143\n","INFO:tensorflow:loss = 0.088314414, step = 1600 (27.690 sec)\n","INFO:tensorflow:global_step/sec: 3.6123\n","INFO:tensorflow:loss = 0.10644252, step = 1700 (27.683 sec)\n","INFO:tensorflow:global_step/sec: 3.61125\n","INFO:tensorflow:loss = 0.25808966, step = 1800 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.61184\n","INFO:tensorflow:loss = 0.1695795, step = 1900 (27.686 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 2000...\n","INFO:tensorflow:Saving checkpoints for 2000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 2000...\n","INFO:tensorflow:global_step/sec: 2.91456\n","INFO:tensorflow:loss = 0.21686031, step = 2000 (34.310 sec)\n","INFO:tensorflow:global_step/sec: 3.61119\n","INFO:tensorflow:loss = 0.13163616, step = 2100 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.6146\n","INFO:tensorflow:loss = 0.15297079, step = 2200 (27.665 sec)\n","INFO:tensorflow:global_step/sec: 3.61395\n","INFO:tensorflow:loss = 0.36925307, step = 2300 (27.671 sec)\n","INFO:tensorflow:global_step/sec: 3.61423\n","INFO:tensorflow:loss = 0.114019275, step = 2400 (27.668 sec)\n","WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 2455 vs previous value: 2455. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n","INFO:tensorflow:global_step/sec: 3.61366\n","INFO:tensorflow:loss = 0.11615274, step = 2500 (27.673 sec)\n","INFO:tensorflow:global_step/sec: 3.61142\n","INFO:tensorflow:loss = 0.175344, step = 2600 (27.690 sec)\n","INFO:tensorflow:global_step/sec: 3.61317\n","INFO:tensorflow:loss = 0.25271764, step = 2700 (27.677 sec)\n","INFO:tensorflow:global_step/sec: 3.61123\n","INFO:tensorflow:loss = 0.17318177, step = 2800 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.6119\n","INFO:tensorflow:loss = 0.06239468, step = 2900 (27.686 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 3000...\n","INFO:tensorflow:Saving checkpoints for 3000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 3000...\n","INFO:tensorflow:global_step/sec: 2.90926\n","INFO:tensorflow:loss = 0.18829328, step = 3000 (34.373 sec)\n","INFO:tensorflow:global_step/sec: 3.61335\n","INFO:tensorflow:loss = 0.05111843, step = 3100 (27.675 sec)\n","INFO:tensorflow:global_step/sec: 3.61409\n","INFO:tensorflow:loss = 0.24524082, step = 3200 (27.669 sec)\n","INFO:tensorflow:global_step/sec: 3.61363\n","INFO:tensorflow:loss = 0.13320367, step = 3300 (27.673 sec)\n","INFO:tensorflow:global_step/sec: 3.61276\n","INFO:tensorflow:loss = 0.08376819, step = 3400 (27.679 sec)\n","INFO:tensorflow:global_step/sec: 3.61385\n","INFO:tensorflow:loss = 0.12795402, step = 3500 (27.671 sec)\n","INFO:tensorflow:global_step/sec: 3.61437\n","INFO:tensorflow:loss = 0.19335483, step = 3600 (27.667 sec)\n","INFO:tensorflow:global_step/sec: 3.61263\n","INFO:tensorflow:loss = 0.15017633, step = 3700 (27.681 sec)\n","INFO:tensorflow:global_step/sec: 3.61442\n","INFO:tensorflow:loss = 0.065538555, step = 3800 (27.667 sec)\n","INFO:tensorflow:global_step/sec: 3.61276\n","INFO:tensorflow:loss = 0.22523241, step = 3900 (27.679 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 4000...\n","INFO:tensorflow:Saving checkpoints for 4000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 4000...\n","INFO:tensorflow:global_step/sec: 2.9232\n","INFO:tensorflow:loss = 0.2727836, step = 4000 (34.209 sec)\n","INFO:tensorflow:global_step/sec: 3.61305\n","INFO:tensorflow:loss = 0.14509568, step = 4100 (27.678 sec)\n","INFO:tensorflow:global_step/sec: 3.6106\n","INFO:tensorflow:loss = 0.11440369, step = 4200 (27.696 sec)\n","INFO:tensorflow:global_step/sec: 3.61191\n","INFO:tensorflow:loss = 0.033119533, step = 4300 (27.687 sec)\n","INFO:tensorflow:global_step/sec: 3.61145\n","INFO:tensorflow:loss = 0.080134474, step = 4400 (27.689 sec)\n","INFO:tensorflow:global_step/sec: 3.61287\n","INFO:tensorflow:loss = 0.17300154, step = 4500 (27.679 sec)\n","INFO:tensorflow:global_step/sec: 3.61119\n","INFO:tensorflow:loss = 0.1248466, step = 4600 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.61108\n","INFO:tensorflow:loss = 0.07112312, step = 4700 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.61208\n","INFO:tensorflow:loss = 0.15397121, step = 4800 (27.685 sec)\n","INFO:tensorflow:global_step/sec: 3.61207\n","INFO:tensorflow:loss = 0.10771375, step = 4900 (27.685 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5000...\n","INFO:tensorflow:Saving checkpoints for 5000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5000...\n","INFO:tensorflow:global_step/sec: 2.91099\n","INFO:tensorflow:loss = 0.1193791, step = 5000 (34.352 sec)\n","INFO:tensorflow:global_step/sec: 3.60977\n","INFO:tensorflow:loss = 0.21738218, step = 5100 (27.703 sec)\n","INFO:tensorflow:global_step/sec: 3.61319\n","INFO:tensorflow:loss = 0.20238821, step = 5200 (27.676 sec)\n","INFO:tensorflow:global_step/sec: 3.61315\n","INFO:tensorflow:loss = 0.19861339, step = 5300 (27.677 sec)\n","INFO:tensorflow:global_step/sec: 3.61211\n","INFO:tensorflow:loss = 0.21896757, step = 5400 (27.684 sec)\n","INFO:tensorflow:global_step/sec: 3.61296\n","INFO:tensorflow:loss = 0.085625894, step = 5500 (27.678 sec)\n","INFO:tensorflow:global_step/sec: 3.61265\n","INFO:tensorflow:loss = 0.13051562, step = 5600 (27.681 sec)\n","INFO:tensorflow:global_step/sec: 3.61214\n","INFO:tensorflow:loss = 0.20694159, step = 5700 (27.684 sec)\n","INFO:tensorflow:global_step/sec: 3.61286\n","INFO:tensorflow:loss = 0.2051484, step = 5800 (27.679 sec)\n","INFO:tensorflow:global_step/sec: 3.61256\n","INFO:tensorflow:loss = 0.07956552, step = 5900 (27.681 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 6000...\n","INFO:tensorflow:Saving checkpoints for 6000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 6000...\n","INFO:tensorflow:global_step/sec: 2.91996\n","INFO:tensorflow:loss = 0.15709966, step = 6000 (34.247 sec)\n","INFO:tensorflow:global_step/sec: 3.61144\n","INFO:tensorflow:loss = 0.29266432, step = 6100 (27.690 sec)\n","INFO:tensorflow:global_step/sec: 3.61247\n","INFO:tensorflow:loss = 0.10504168, step = 6200 (27.682 sec)\n","INFO:tensorflow:global_step/sec: 3.61332\n","INFO:tensorflow:loss = 0.17424543, step = 6300 (27.676 sec)\n","INFO:tensorflow:global_step/sec: 3.61141\n","INFO:tensorflow:loss = 0.0666467, step = 6400 (27.690 sec)\n","INFO:tensorflow:global_step/sec: 3.61196\n","INFO:tensorflow:loss = 0.091589205, step = 6500 (27.686 sec)\n","INFO:tensorflow:global_step/sec: 3.61004\n","INFO:tensorflow:loss = 0.15319514, step = 6600 (27.700 sec)\n","INFO:tensorflow:global_step/sec: 3.61116\n","INFO:tensorflow:loss = 0.14827538, step = 6700 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.61053\n","INFO:tensorflow:loss = 0.12459359, step = 6800 (27.697 sec)\n","INFO:tensorflow:global_step/sec: 3.61135\n","INFO:tensorflow:loss = 0.19907089, step = 6900 (27.690 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 7000...\n","INFO:tensorflow:Saving checkpoints for 7000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 7000...\n","INFO:tensorflow:global_step/sec: 2.96834\n","INFO:tensorflow:loss = 0.08872235, step = 7000 (33.689 sec)\n","WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 7019 vs previous value: 7019. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n","WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 7080 vs previous value: 7080. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n","INFO:tensorflow:global_step/sec: 3.60905\n","INFO:tensorflow:loss = 0.12911941, step = 7100 (27.708 sec)\n","WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 7101 vs previous value: 7101. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n","INFO:tensorflow:global_step/sec: 3.61186\n","INFO:tensorflow:loss = 0.06348806, step = 7200 (27.686 sec)\n","INFO:tensorflow:global_step/sec: 3.61342\n","INFO:tensorflow:loss = 0.1160892, step = 7300 (27.675 sec)\n","INFO:tensorflow:global_step/sec: 3.61107\n","INFO:tensorflow:loss = 0.04131021, step = 7400 (27.693 sec)\n","INFO:tensorflow:global_step/sec: 3.61283\n","INFO:tensorflow:loss = 0.16131039, step = 7500 (27.679 sec)\n","INFO:tensorflow:global_step/sec: 3.61289\n","INFO:tensorflow:loss = 0.03197098, step = 7600 (27.679 sec)\n","INFO:tensorflow:global_step/sec: 3.61236\n","INFO:tensorflow:loss = 0.17901184, step = 7700 (27.683 sec)\n","INFO:tensorflow:global_step/sec: 3.61197\n","INFO:tensorflow:loss = 0.11784506, step = 7800 (27.686 sec)\n","INFO:tensorflow:global_step/sec: 3.61306\n","INFO:tensorflow:loss = 0.09346032, step = 7900 (27.677 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 8000...\n","INFO:tensorflow:Saving checkpoints for 8000 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 8000...\n","INFO:tensorflow:global_step/sec: 2.89328\n","INFO:tensorflow:loss = 0.2355061, step = 8000 (34.563 sec)\n","INFO:tensorflow:global_step/sec: 3.61118\n","INFO:tensorflow:loss = 0.056324553, step = 8100 (27.692 sec)\n","INFO:tensorflow:global_step/sec: 3.6128\n","INFO:tensorflow:loss = 0.20725392, step = 8200 (27.679 sec)\n","INFO:tensorflow:global_step/sec: 3.61265\n","INFO:tensorflow:loss = 0.052404046, step = 8300 (27.681 sec)\n","INFO:tensorflow:global_step/sec: 3.61101\n","INFO:tensorflow:loss = 0.13225561, step = 8400 (27.693 sec)\n","INFO:tensorflow:global_step/sec: 3.61171\n","INFO:tensorflow:loss = 0.111375175, step = 8500 (27.688 sec)\n","INFO:tensorflow:global_step/sec: 3.61316\n","INFO:tensorflow:loss = 0.1966412, step = 8600 (27.677 sec)\n","INFO:tensorflow:global_step/sec: 3.6124\n","INFO:tensorflow:loss = 0.18102443, step = 8700 (27.682 sec)\n","INFO:tensorflow:global_step/sec: 3.61093\n","INFO:tensorflow:loss = 0.1219514, step = 8800 (27.694 sec)\n","INFO:tensorflow:global_step/sec: 3.6121\n","INFO:tensorflow:loss = 0.16112436, step = 8900 (27.685 sec)\n","INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 8975...\n","INFO:tensorflow:Saving checkpoints for 8975 into bert_output/model.ckpt.\n","INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 8975...\n","INFO:tensorflow:Loss for final step: 0.20638268.\n","Training took time  0:43:30.664367\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2qMZKXzuFoL9"},"source":["## Testing (evaluating)"]},{"cell_type":"markdown","metadata":{"id":"GCvkTNhqNTDT"},"source":["### Configure validation .TFRecord file "]},{"cell_type":"code","metadata":{"id":"8Cj3cX0xE0tz"},"source":["fileToEval = os.path.join('bert_output', \"test.tf_record\") # ~9 mb in size\n","\n","# create file if not exist\n","if not os.path.exists(fileToEval):\n","    open(fileToEval, 'w').close()\n","\n","evalInput = createBertInput(test) # write test set examples to TFRecord file\n","# convert examples -> features for BERT eval\n","file_based_convert_examples_to_features(\n","   evalInput, maxSeqLength, tokenizer, fileToEval)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PaiEZzpxNW4w"},"source":["### Configure evaluation function"]},{"cell_type":"code","metadata":{"id":"6ZYXafJjNbuw"},"source":["evalFunction = file_based_input_fn_builder(\n","    input_file=fileToEval,\n","    seq_length=maxSeqLength,\n","    is_training=False, # since validation not train\n","    drop_remainder=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gv6lncE6NfA3"},"source":["### Begin evaluating test set!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GjgW8NBE2-n","executionInfo":{"status":"ok","timestamp":1607710766859,"user_tz":300,"elapsed":70398,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"c2504414-eef5-4f70-feeb-3dc53324a9a3"},"source":["evaluation = estimator.evaluate(input_fn=evalFunction, steps=None)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n","INFO:tensorflow:num_labels:6;logits:Tensor(\"loss/BiasAdd:0\", shape=(None, 6), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(None, 6), dtype=float32)\n","INFO:tensorflow:**** Trainable Variables ****\n","WARNING:tensorflow:From <ipython-input-28-f943af317e62>:127: auc (from tensorflow.python.ops.metrics_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The value of AUC returned by this may race with the update so this is deprecated. Please use tf.keras.metrics.AUC instead.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2020-12-11T18:18:38Z\n","INFO:tensorflow:Graph was finalized.\n","INFO:tensorflow:Restoring parameters from bert_output/model.ckpt-8975\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","INFO:tensorflow:Inference Time : 46.66910s\n","INFO:tensorflow:Finished evaluation at 2020-12-11-18:19:25\n","INFO:tensorflow:Saving dict for global step 8975: 0 = 0.5102444, 1 = 0.5612399, 2 = 0.534467, 3 = 0.5092827, 4 = 0.5490346, 5 = 0.52136266, accuracy = 0.96273553, eval_loss = 0.19433822, global_step = 8975, loss = 0.19434963\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 8975: bert_output/model.ckpt-8975\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"whw_-XSPNpB6"},"source":["# put test metrics into .txt file\n","evalOutput = os.path.join(\"bert_output\", \"evalOutput.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FlPuHRGE--0","executionInfo":{"status":"ok","timestamp":1607710766860,"user_tz":300,"elapsed":68916,"user":{"displayName":"Elisa D","photoUrl":"","userId":"14931669874387958441"}},"outputId":"f8749fa8-35b2-45f2-88b4-43b08d2cf3c8"},"source":["# write to .txt file\n","with tf.compat.v1.gfile.GFile(evalOutput, \"w\") as w:\n","    tf.compat.v1.logging.info(\"Evaluating test set...\")\n","    for metric in sorted(evaluation.keys()):\n","        tf.compat.v1.logging.info(\"  %s = %s\", metric, str(evaluation[metric]))\n","        w.write(\"%s = %s\\n\" % (metric, str(evaluation[metric])))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:***** Eval results *****\n","INFO:tensorflow:  0 = 0.5102444\n","INFO:tensorflow:  1 = 0.5612399\n","INFO:tensorflow:  2 = 0.534467\n","INFO:tensorflow:  3 = 0.5092827\n","INFO:tensorflow:  4 = 0.5490346\n","INFO:tensorflow:  5 = 0.52136266\n","INFO:tensorflow:  accuracy = 0.96273553\n","INFO:tensorflow:  eval_loss = 0.19433822\n","INFO:tensorflow:  global_step = 8975\n","INFO:tensorflow:  loss = 0.19434963\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a0_MuQTpA1TI"},"source":["## Unmount drive"]},{"cell_type":"code","metadata":{"id":"Ln7wxmW8puoc"},"source":["# unmount drive when done\n","from google.colab import drive\n","drive.flush_and_unmount()"],"execution_count":null,"outputs":[]}]}